# ğŸ§  Semantic Hybrid Retrieval System for Academic Funding Discovery

This project builds an intelligent retrieval system that helps researchers discover relevant NSF funding opportunities based on their research ideas.  
It combines **keyword-based retrieval (BM25)** with **semantic retrieval (SBERT + FAISS)** and adds a **Large Language Model (LLM) for explainability**.

This hybrid approach produces more meaningful search results and provides natural-language explanations for *why* the retrieved grants are relevant.

---

## ğŸ” Why This Project?

Finding research funding often depends on exact keyword matches, which miss many relevant opportunities.  
This system goes **beyond keyword matching** by understanding:

- Semantic meaning  
- Research context  
- Topic relevance  

The LLM then explains *why* a result is relevant â€” solving a real problem researchers face.

---

# ğŸ¯ Objectives

- Build a **hybrid semantic search engine** for NSF grants  
- Improve over BM25 baseline using embeddings  
- Add **explainable retrieval** using an LLM  
- Evaluate retrieval performance using:
  - Precision@5  
  - nDCG@5  
  - MRR  
  - Human vs LLM agreement  

---

# âš™ï¸ Technical Workflow

### **1. Data Preparation**

Dataset source: The original full dataset is available on Kaggle:

**NSF Awards Dataset (kaggle)**:https://www.kaggle.com/datasets/xiyaocheng/nsf-awards-dataset?select=nsf_dataset.csv
**It is very large (~300MB), it cannot be hosted directly in this GitHub repository.**

The processed clean data file is provided in Data folder as google drive link.


Steps performed (documented in the evaluation notebook):

1. Load dataset and remove duplicates + missing abstracts  
2. Normalize NSF program codes  
3. Map programs into 4 categories:  
   - **BIO â€” Biological Sciences**  
   - **CNS â€” Computer and Network Systems**  
   - **IIS â€” Information & Intelligent Systems**  
   - **OTHER â€” All remaining programs**  
4. Downsample large classes for balance  
5. Export final cleaned file: `nsf_grants_clean.csv`

---

### **2. Retrieval Methodology**

#### BM25 Baseline Retrieval
- Tokenize abstracts  
- Build BM25 index  
- Retrieve top-k grants  
- Compute baseline IR metrics:
  - Precision@5  
  - MRR  
  - nDCG  

---

#### Semantic Retrieval (SBERT + FAISS)
- Encode abstracts using **Sentence-BERT (all-MiniLM-L6-v2)**  
- Store vectors in FAISS index  
- Perform dense retrieval    

---

#### Hybrid Retrieval
-BM25 + SBERT combined:
This improves both **recall** and **semantic matching**, especially when query wording differs from the grant abstract.

---

### **3. LLM Explainability (RAG Pipeline)**
For each retrieved grant, the LLM:

- Assigns a binary relevance label  
- Generates a short explanation  
- Justifies its label  

This adds **interpretability**, which is essential for researchers.

---

### **4. Evaluation Framework**
Includes both **quantitative** and **qualitative** evaluation:

#### Quantitative
| Metric | Meaning |
|--------|---------|
| **Precision@5** | How many of the top-5 results are relevant |
| **MRR** | Rank position of first relevant document |
| **nDCG** | Penalizes relevant items appearing lower in ranking | 
- Human vs LLM agreement score- consistency between human labels and LLM labels  

#### Qualitative
- **Human relevance labels used as ground truth** for validating retrieval  
- **LLM-generated natural-language explanations** for each retrieved result  
- **Assessment of explanation faithfulness and clarity** to ensure the reasoning matches the grant content   

---

# ğŸ“ Folder Structure

```plaintext
Semantic-hybrid-retrieval-for-funding-discovery/
â”‚
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ README.md                   # Link to the original and cleaned data(large files)
â”‚
â”œâ”€â”€ app.py                          # Streamlit UI for search + evaluation
â”œâ”€â”€ retrieval_core.py               # Hybrid retrieval + evaluation logic
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ evaluation.ipynb             # Full evaluation pipeline
|
â”œâ”€â”€ images/                           # UI screenshots
â”‚   â”œâ”€â”€ ui_demo_overview.png          #An overview of the UI interface
â”‚   â”œâ”€â”€ ui_search_page.png            # Search page screenshot
â”‚   â”œâ”€â”€ ui_evaluation_page.png        #Evaluation tab screenshot with results
â”‚   â””â”€â”€ ui_results_detail.png         #Evaluation metrics shown
|          
|
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       
â””â”€â”€ .gitignore  


---

# Why Two Code Components?

### âœ”ï¸ `retrieval_core.py`
**Purpose:**  
Contains all retrieval logic (backend) needed by the Streamlit UI.

Includes:
- BM25 setup  
- SBERT + FAISS  
- Hybrid ranking  
- LLM relevance + explanations  
- Evaluation metrics  

---

### âœ”ï¸ `app.py`
**Purpose:**  
Interactive Streamlit interface enabling (frontend):

- Query search  
- Real-time retrieval  
- LLM-generated explanations  
- Evaluation page (metrics + human labels + LLM labels)

This file contains only **UI logic**, with all computation done in `retrieval_core.py`.

---

### âœ”ï¸ `evaluation.ipynb` (Evaluation Notebook)
**Why this exists separately:**

The notebook documents the **full research workflow**:

- Data cleaning  
- Category mapping  
- BM25 baseline experiments  
- SBERT embedding generation  
- Hybrid ranking analysis  
- Metric comparison  

The app focuses on *deployment*, while the notebook focuses on *methodology and evaluation*.  
They are not duplicates â€” they serve different purposes.

---
### SETUP 

# 1. Install dependencies
pip install -r requirements.txt

# 2. Set your OpenAI API key
# -------------------------
# Mac / Linux
export OPENAI_API_KEY="your-key-here"

# Windows PowerShell
setx OPENAI_API_KEY "your-key-here"

# (Restart terminal after running setx)

# 3. Run the Streamlit app
streamlit run app.py

You will see:

- Search tab â†’ semantic grant search  
- Evaluation tab â†’ metrics + human/LLM labels  

---

# âœ… Summary

This project delivers:

- A hybrid semantic retrieval engine  
- Explainable funding recommendations  
- Evaluation pipeline with human + LLM labels  
- A clean, interactive UI for demonstration  

---