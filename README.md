# Semantic-hybrid-retrieval-for-funding-discovery

ğŸ§  Semantic Hybrid Retrieval System for Academic Funding Discovery

This project builds an intelligent retrieval system that helps researchers discover relevant academic funding opportunities (e.g., NSF grants) based on their research ideas.
By combining keyword-based retrieval (BM25) and semantic retrieval (Sentence-BERT + FAISS), it creates a hybrid Retrieval-Augmented Generation (RAG) pipeline for smarter and explainable funding discovery.

ğŸ§© Project Overview

Finding funding opportunities can be tedious when relying on keyword search.
This system uses semantic embeddings and vector databases to go beyond simple word matching â€” it understands meaning and context.
By integrating a Large Language Model (LLM), it provides human-readable explanations for why certain grants match a given query.

ğŸ¯ Objectives

Develop a hybrid grant retrieval system combining sparse and dense retrieval.

Improve accuracy over traditional BM25 keyword search.

Integrate RAG and LLMs for contextual understanding and interpretability.

Evaluate retrieval performance using standard IR metrics (Precision, nDCG, MRR).

âš™ï¸ Technical Workflow
1. Data Preparation

Source: NSF Award Abstracts Dataset (Kaggle)

Tasks:

Load and clean abstracts (remove formatting, symbols, etc.)

Standardize NSF program categories (BIO, CNS, IIS, OTHER)

Store processed text for downstream retrieval

2. Baseline Sparse Retrieval (BM25)

Implement TF-IDF/BM25 to retrieve top-k grants for sample researcher queries.

Evaluate ranking with metrics:

Precision@5

nDCG@5

MRR@5

Acts as a benchmark for semantic retrieval improvements.

3. Dense Semantic Retrieval

Use Sentence-BERT (SBERT) to embed both abstracts and queries into vector space.

Store embeddings in a FAISS vector database for efficient similarity search.

Compare semantic retrieval performance against BM25 baseline.

4. Hybrid RAG Pipeline

Combine semantic retrieval results with an LLM (like GPT) for contextual reasoning.

The LLM:

Generates short explanations for why a grant is relevant.

Helps align userâ€™s research proposal ideas with grant descriptions.

5. Evaluation Framework

Quantitative: Precision@k, nDCG, MRR.

Qualitative: Faithfulness and relevance of LLM-generated explanations.

Comparison: BM25 vs SBERT vs Hybrid retrieval results.

ğŸ§ª Workflow Summary
Dataset (NSF Abstracts)
        â†“
Data Cleaning & Preprocessing
        â†“
BM25 Baseline Retrieval
        â†“
Evaluate Baseline (Precision, nDCG, MRR)
        â†“
SBERT Embeddings + FAISS Index
        â†“
Semantic Retrieval (Top-k Results)
        â†“
RAG Integration (LLM Generates Explanations)
        â†“
Hybrid Evaluation (Quantitative + Qualitative)

ğŸ“‚ Folder Structure
semantic-retrieval-funding/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original NSF dataset from Kaggle
â”‚   â””â”€â”€ processed/          # Cleaned and categorized abstracts
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ INFO556_Project_Update.ipynb  # Main Google Colab notebook
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bm25_baseline.py            # Keyword retrieval
â”‚   â”œâ”€â”€ semantic_retrieval.py       # SBERT + FAISS retrieval
â”‚   â”œâ”€â”€ rag_pipeline.py             # LLM integration for explanation
â”‚   â”œâ”€â”€ evaluation.py               # Precision, nDCG, MRR computations
â”‚   â””â”€â”€ utils.py                    # Helper functions (cleaning, etc.)
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ bm25_metrics.csv
â”‚   â”œâ”€â”€ sbert_metrics.csv
â”‚   â””â”€â”€ visualizations/             # Graphs and comparison charts
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


